{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAagmxIetPqVbHQ0SjE9vz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monteroserra/introduction_python_programming/blob/main/Code_Lab_3_Intro_to_Gemini_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kaggle Intensive LLM Course\n",
        "\n",
        "#### Day1 : [link to youtube video](https://www.youtube.com/watch?v=Na3O4Pkbp-U&list=PLqFaTIg4myu_yKJpvF8WE2JfaG5kGuvoE&index=2)"
      ],
      "metadata": {
        "id": "gyzf4b9pMdav"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n"
      ],
      "metadata": {
        "id": "Iz_x_t__Kqoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY = 'XXX'"
      ],
      "metadata": {
        "id": "G5e9yWKWKqv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "for model in client.models.list():\n",
        "  print(model.name)"
      ],
      "metadata": {
        "id": "qyk3zRqVKqyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=\"find me a good pasta restaurant in Barcelona center\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saA1j6p-Kq1T",
        "outputId": "135a209b-b08d-433a-f2e6-9bca7ebac45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Okay, finding a \"good\" pasta restaurant is subjective, but here are a few well-regarded and popular options in the Barcelona city center, catering to different preferences and price points:\n",
            "\n",
            "**Mid-Range, Authentic Italian & Highly Rated:**\n",
            "\n",
            "*   **Le Cucine Mandarosso:** (Multiple locations, including one near the Born and another near Barceloneta) Consistently praised for its fresh, homemade pasta, generous portions, and authentic Sicilian flavors. They offer a wide variety of pasta shapes and sauces, and their arancini are also a must-try. Expect a lively, slightly rustic atmosphere.\n",
            "    *   **Why it's good:** Fresh pasta, authentic Sicilian flavors, good value, popular with locals.\n",
            "    *   **Best for:** A satisfying and authentic Italian meal, not too formal.\n",
            "    *   **Address Example:** Carrer de Sant Domènec, 19, Ciutat Vella, 08001 Barcelona\n",
            "*   **Da Greco:** (Carrer de Santa Teresa, 10) A classic Italian restaurant that has been serving Barcelona for years. Known for its elegant atmosphere, attentive service, and high-quality ingredients. Their pasta dishes are carefully prepared and feature traditional recipes.\n",
            "    *   **Why it's good:** High quality ingredients, great service, classic vibe.\n",
            "    *   **Best for:** A bit fancier meal, special occasion.\n",
            "    *   **Address:** Carrer de Santa Teresa, 10, 08012 Barcelona\n",
            "\n",
            "**Trendy and Modern:**\n",
            "\n",
            "*   **Cecconi's Barcelona:** (Pg. de Joan Borbó, 14) Located in Soho House, this chic Italian restaurant offers a stylish setting and a menu of classic pasta dishes with a modern twist. It's a great option for a special occasion or a romantic dinner.\n",
            "    *   **Why it's good:** Stylish setting, great location, quality ingredients.\n",
            "    *   **Best for:** A special occasion, trendy vibe, views of the port.\n",
            "    *   **Address:** Pg. de Joan Borbó, 14, Ciutat Vella, 08003 Barcelona\n",
            "\n",
            "**Important Considerations:**\n",
            "\n",
            "*   **Reservations:** Highly recommended, especially for dinner, as these places can be busy.\n",
            "*   **Neighborhood:**  \"Barcelona Center\" is broad.  Born, Gothic Quarter, Eixample, and Barceloneta are all central. Consider which area you'll be in when making your choice.\n",
            "*   **Price:**  Prices can vary significantly. Check menus online or ask when booking.\n",
            "\n",
            "**To Help Me Narrow Down the Best Recommendation:**\n",
            "\n",
            "1.  **What kind of atmosphere are you looking for?** (Casual, romantic, trendy, family-friendly?)\n",
            "2.  **What's your budget?** (Budget-friendly, mid-range, or fine dining?)\n",
            "3.  **Are you looking for a specific type of pasta dish or Italian cuisine?** (e.g., Roman-style, Sicilian, seafood-focused?)\n",
            "\n",
            "Let me know, and I can provide a more tailored recommendation!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def gemini_replier(my_question):\n",
        "    response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=my_question)\n",
        "    print(response.text)\n",
        "\n",
        "    return response.text\n",
        "\n",
        "\n",
        "# test our function\n",
        "\n",
        "my_question = \"what's the best pasta restaurant?\"\n",
        "gemini_replier(my_question)\n",
        "\n",
        "\n",
        "####\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "eVhrz1dyMgoY",
        "outputId": "3ed66403-c429-47c6-d3ee-31bd08308b82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am doing well, thank you for asking! As a large language model, I don't experience emotions or feelings in the same way humans do. But I am functioning optimally and ready to assist you with your requests. How can I help you today?\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I am doing well, thank you for asking! As a large language model, I don't experience emotions or feelings in the same way humans do. But I am functioning optimally and ready to assist you with your requests. How can I help you today?\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## create a dynamic prompt\n",
        "name = 'nacho'\n",
        "\n",
        "city = 'Barcelona'\n",
        "my_question = \"what's the best pasta restaurant?\"\n",
        "\n",
        "my_prompt = f\"you are an expert in cusine, answer this client: {name}, that lives in {city}, and has this question: {my_question}\"\n",
        "\n",
        "print(my_prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ_Hq_9uO2xI",
        "outputId": "6d998531-aac1-49b1-849e-f7065750e734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "you are an expert in cusine, answer this client: nacho, that lives in Barcelona, and has this question: what's the best pasta restaurant?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "from google.genai import types\n",
        "\n",
        "def create_prompt(name, city, my_question):\n",
        "    my_prompt = f\"you are an expert in cusine, answer this client: {name}, that lives in {city}, and has this question: {my_question}\"\n",
        "    return my_prompt\n",
        "\n",
        "def gemini_replier(my_prompt):\n",
        "    response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\",\n",
        "    contents=my_prompt)\n",
        "    print(response.text)\n",
        "\n",
        "    return response.text\n",
        "\n",
        "name = 'nacho'\n",
        "city = 'Barcelona'\n",
        "my_question = \"what's the best pasta restaurant?\"\n",
        "\n",
        "my_prompt = create_prompt(name, city, my_question)\n",
        "gemini_replier(my_prompt)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "kHzamp19PPiG",
        "outputId": "01e198e6-0d1a-46e6-8317-d65ccdbe1135"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hola Nacho!\n",
            "\n",
            "As an expert in cuisine, especially when it comes to finding the perfect pasta in Barcelona, I'm happy to help! To give you the *best* recommendation, I need a little more information about what you're looking for. Barcelona has a fantastic and varied pasta scene!\n",
            "\n",
            "**To help me narrow it down, tell me:**\n",
            "\n",
            "*   **What's your budget?** (e.g., budget-friendly, mid-range, splurge-worthy)\n",
            "*   **What kind of atmosphere are you after?** (e.g., romantic and cozy, lively and casual, modern and trendy)\n",
            "*   **What type of pasta dishes are you craving?** (e.g., classic Italian, inventive and modern, seafood-focused, vegetarian-friendly, specific pasta shapes like ravioli or gnocchi)\n",
            "*   **What neighborhood are you in or willing to travel to?** (e.g., Gràcia, El Born, Barceloneta, Eixample)\n",
            "*   **Do you care about the wine list?** (Important if you want a great pairing!)\n",
            "\n",
            "**In the meantime, here are a few general recommendations based on popularity and reputation, covering a range of styles:**\n",
            "\n",
            "*   **For a classic Italian experience (and consistently excellent pasta):**\n",
            "    *   **Le Cucine Mandarosso (Born):** Known for authentic Neapolitan dishes and fresh pasta made in-house. A classic choice for a reason! Reservations recommended.\n",
            "*   **For a more modern and inventive take on Italian cuisine:**\n",
            "    *   **Xemei (Poble Sec):** Not exclusively a pasta restaurant, but their Venetian-inspired pasta dishes are highly acclaimed. A bit further out but worth the trip. Reservations essential.\n",
            "*   **For excellent value and consistently good pasta:**\n",
            "    *   **Da Greco (Eixample):** A well-established Italian restaurant with a large menu and a good selection of pasta dishes. A reliable choice.\n",
            "*   **For a romantic and intimate setting:**\n",
            "    *   **Bacaro (El Born):** Small plates and excellent handmade pasta. It is a very popular place.\n",
            "*   **For fresh pasta and a local atmosphere:**\n",
            "    *   **Sensi Noodles (Gothic Quarter):** This restaurant is known for its friendly service and fresh, handmade pasta dishes. They offer a variety of options, including vegetarian and gluten-free choices. The portion are very generous, and it is great for sharing.\n",
            "\n",
            "Once I have a better idea of your preferences, I can give you a much more personalized and spot-on recommendation!\n",
            "\n",
            "¡Espero tu respuesta, Nacho! Let me know what you're in the mood for, and I'll help you find the perfect pasta place in Barcelona.\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hola Nacho!\\n\\nAs an expert in cuisine, especially when it comes to finding the perfect pasta in Barcelona, I'm happy to help! To give you the *best* recommendation, I need a little more information about what you're looking for. Barcelona has a fantastic and varied pasta scene!\\n\\n**To help me narrow it down, tell me:**\\n\\n*   **What's your budget?** (e.g., budget-friendly, mid-range, splurge-worthy)\\n*   **What kind of atmosphere are you after?** (e.g., romantic and cozy, lively and casual, modern and trendy)\\n*   **What type of pasta dishes are you craving?** (e.g., classic Italian, inventive and modern, seafood-focused, vegetarian-friendly, specific pasta shapes like ravioli or gnocchi)\\n*   **What neighborhood are you in or willing to travel to?** (e.g., Gràcia, El Born, Barceloneta, Eixample)\\n*   **Do you care about the wine list?** (Important if you want a great pairing!)\\n\\n**In the meantime, here are a few general recommendations based on popularity and reputation, covering a range of styles:**\\n\\n*   **For a classic Italian experience (and consistently excellent pasta):**\\n    *   **Le Cucine Mandarosso (Born):** Known for authentic Neapolitan dishes and fresh pasta made in-house. A classic choice for a reason! Reservations recommended.\\n*   **For a more modern and inventive take on Italian cuisine:**\\n    *   **Xemei (Poble Sec):** Not exclusively a pasta restaurant, but their Venetian-inspired pasta dishes are highly acclaimed. A bit further out but worth the trip. Reservations essential.\\n*   **For excellent value and consistently good pasta:**\\n    *   **Da Greco (Eixample):** A well-established Italian restaurant with a large menu and a good selection of pasta dishes. A reliable choice.\\n*   **For a romantic and intimate setting:**\\n    *   **Bacaro (El Born):** Small plates and excellent handmade pasta. It is a very popular place.\\n*   **For fresh pasta and a local atmosphere:**\\n    *   **Sensi Noodles (Gothic Quarter):** This restaurant is known for its friendly service and fresh, handmade pasta dishes. They offer a variety of options, including vegetarian and gluten-free choices. The portion are very generous, and it is great for sharing.\\n\\nOnce I have a better idea of your preferences, I can give you a much more personalized and spot-on recommendation!\\n\\n¡Espero tu respuesta, Nacho! Let me know what you're in the mood for, and I'll help you find the perfect pasta place in Barcelona.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompting\n",
        "This section contains some prompts from the chapter for you to try out directly in the API. Try changing the text here to see how each prompt performs with different instructions, more examples, or any other changes you can think of."
      ],
      "metadata": {
        "id": "zFh08UtuVlgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zero-shot\n",
        "Zero-shot prompts are prompts that describe the request for the model directly."
      ],
      "metadata": {
        "id": "P4A6jg1wVWai"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_config = types.GenerateContentConfig(\n",
        "    temperature=0.1,\n",
        "    top_p=1,\n",
        "    max_output_tokens=5,\n",
        ")\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=model_config,\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vdFcIMwU8nZ",
        "outputId": "0bdb01a3-ebfb-4b86-841e-8d07387101f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POSITIVE\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Enum mode\n",
        "The models are trained to generate text, and while the Gemini 2.0 models are great at following instructions, other models can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, but sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards. See this prompt in AI Studio for an example.\n",
        "\n",
        "The Gemini API has an Enum mode feature that allows you to constrain the output to a fixed set of values."
      ],
      "metadata": {
        "id": "fIDx76jRVrOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ),\n",
        "    contents=zero_shot_prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNd8YUFzVsMw",
        "outputId": "b3a74263-c259-48d9-d9e0-063425962ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When using constrained output like an enum, the Python SDK will attempt to convert the model's text response into a Python object automatically. It's stored in the response.parsed field."
      ],
      "metadata": {
        "id": "WuEMhau8VvYh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enum_response = response.parsed\n",
        "print(enum_response)\n",
        "print(type(enum_response))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51XRqcUUVvlp",
        "outputId": "e2d66043-a1d8-4fa6-a550-532eb8cf8690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment.POSITIVE\n",
            "<enum 'Sentiment'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aCLJiXFbVvoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#One-shot and few-shot\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt."
      ],
      "metadata": {
        "id": "9_MHo71pWDIa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YCy7tTO0WED4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON mode\n",
        "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's JSON mode. This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
      ],
      "metadata": {
        "id": "cwdiVmkhWI2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ),\n",
        "    contents=\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U-xlHQUWJyL",
        "outputId": "b3ec681e-2c7d-4d40-8334-69e4dac0b771"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"size\": \"large\",\n",
            "  \"ingredients\": [\"apple\", \"chocolate\"],\n",
            "  \"type\": \"dessert\"\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chain of Thought (CoT)\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "Models like the Gemini family are trained to be \"chatty\" or \"thoughtful\" and will provide reasoning steps without prompting, so for this simple example you can ask the model to be more direct in the prompt to force a non-reasoning response. Try re-running this step if the model gets lucky and gets the answer correct on the first try."
      ],
      "metadata": {
        "id": "FX29yY3JWT55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer directly.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JH3pcNCTWPgI",
        "outputId": "df01a6eb-c1e6-442d-d478-3b58069add10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "52\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now try the same approach, but indicate to the model that it should \"think step by step\"."
      ],
      "metadata": {
        "id": "AC0HiY9TWbvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
        "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "TtUgcA7kWPjI",
        "outputId": "d1e56db4-1f49-4819-d9fa-6252df30a43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Here's how to solve the problem step-by-step:\n\n1.  **Find the age difference:** When you were 4, your partner was 3 times your age, meaning they were 4 * 3 = 12 years old.\n\n2.  **Calculate the age difference:** The age difference between you and your partner is 12 - 4 = 8 years.\n\n3.  **Determine the partner's current age:** Since the age difference remains the same, your partner is currently 20 + 8 = 28 years old.\n\n**Therefore, your partner is currently 28 years old.**"
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ReAct: Reason and act\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the \"Prompting\" whitepaper.\n",
        "\n",
        "To try this out with the Wikipedia search engine, check out the Searching Wikipedia with ReAct cookbook example."
      ],
      "metadata": {
        "id": "zaBh0uohWnlX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "S7rhyAt4WPl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use stop_sequences to end the generation process. The steps are Thought, Action, Observation, in that order."
      ],
      "metadata": {
        "id": "pzQVQOw2W7_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "# You will perform the Action; so generate up to, but not including, the Observation.\n",
        "react_config = types.GenerateContentConfig(\n",
        "    stop_sequences=[\"\\nObservation\"],\n",
        "    system_instruction=model_instructions + example1 + example2,\n",
        ")\n",
        "\n",
        "# Create a chat that has the model instructions and examples pre-seeded.\n",
        "react_chat = client.chats.create(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=react_config,\n",
        ")\n",
        "\n",
        "resp = react_chat.send_message(question)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9i5-WCTkW6PQ",
        "outputId": "a69577ad-7b30-46ec-daa4-a6f2c8310a1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the \"transformers NLP paper\" and then find the author that is youngest.\n",
            "\n",
            "Action 1\n",
            "<search>transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can perform this research yourself and supply it back to the model."
      ],
      "metadata": {
        "id": "IdoTFT3KXGIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TtS1PImW6Yj",
        "outputId": "1d04228c-b384-4a1b-84cb-7b5d36d81fe8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 2\n",
            "Now I need to find the youngest author from the list Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin. I will search for each of their ages to find the youngest one.\n",
            "\n",
            "Action 2\n",
            "<search>Ashish Vaswani age</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process repeats until the <finish> action is reached. You can continue running this yourself if you like, or try the Wikipedia example to see a fully automated ReAct system at work."
      ],
      "metadata": {
        "id": "eUiwjnySXl_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Thinking mode\n",
        "The experiemental Gemini Flash 2.0 \"Thinking\" model has been trained to generate the \"thinking process\" the model goes through as part of its response. As a result, the Flash Thinking model is capable of stronger reasoning capabilities in its responses.\n",
        "\n",
        "Using a \"thinking mode\" model can provide you with high-quality responses without needing specialised prompting like the previous approaches. One reason this technique is effective is that you induce the model to generate relevant information (\"brainstorming\", or \"thoughts\") that is then used as part of the context in which the final response is generated.\n",
        "\n",
        "Note that when you use the API, you get the final response from the model, but the thoughts are not captured. To see the intermediate thoughts, try out the thinking mode model in AI Studio."
      ],
      "metadata": {
        "id": "rd1EkAanXoj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "from IPython.display import Markdown, clear_output\n",
        "\n",
        "\n",
        "response = client.models.generate_content_stream(\n",
        "    model='gemini-2.0-flash-thinking-exp',\n",
        "    contents='Who was the youngest author listed on the transformers NLP paper?',\n",
        ")\n",
        "\n",
        "buf = io.StringIO()\n",
        "for chunk in response:\n",
        "    buf.write(chunk.text)\n",
        "    # Display the response as it is streamed\n",
        "    print(chunk.text, end='')\n",
        "\n",
        "# And then render the finished response as formatted markdown.\n",
        "clear_output()\n",
        "\n",
        "Markdown(buf.getvalue())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "id": "j-PczitiXKxn",
        "outputId": "2c01e5e1-8096-46cb-9f56-04b445a939f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The youngest author listed on the \"Attention is All You Need\" (Transformers) paper is **Aidan N. Gomez**.\n\nWhile precise birthdates aren't readily available for all authors, it's well-known that at the time of the paper's publication, **Aidan N. Gomez was a PhD student at the University of Toronto**, working in Geoffrey Hinton's lab.  The other authors were researchers at Google Brain.  PhD students are generally younger than established researchers at major companies like Google.\n\nTherefore, based on career stage at the time of publication, **Aidan N. Gomez is considered the youngest author on the Transformers paper.**"
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code prompting\n",
        "\n",
        "## Generating code\n",
        "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
        "\n",
        "It's important to be aware that since LLMs can make mistakes, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses."
      ],
      "metadata": {
        "id": "mudE1bYVYJnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Gemini models love to talk, so it helps to specify they stick to the code if that\n",
        "# is all that you want.\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=types.GenerateContentConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ),\n",
        "    contents=code_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "6y6tu7LgYHQJ",
        "outputId": "0284b346-9419-4c1f-9045-a0dee82c9ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  \"\"\"Calculates the factorial of a non-negative integer.\n\n  Args:\n    n: A non-negative integer.\n\n  Returns:\n    The factorial of n (n!), or 1 if n is 0.\n    Raises ValueError if n is negative.\n  \"\"\"\n  if n < 0:\n    raise ValueError(\"Factorial is not defined for negative numbers.\")\n  elif n == 0:\n    return 1\n  else:\n    result = 1\n    for i in range(1, n + 1):\n      result *= i\n    return result\n```"
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code execution\n",
        "The Gemini API can automatically run generated code too, and will return the output."
      ],
      "metadata": {
        "id": "BP2g1HxNYa7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint\n",
        "\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=[types.Tool(code_execution=types.ToolCodeExecution())],\n",
        ")\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Generate the first 14 odd prime numbers, then calculate their sum.\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    config=config,\n",
        "    contents=code_exec_prompt)\n",
        "\n",
        "for part in response.candidates[0].content.parts:\n",
        "  pprint(part.to_json_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCoVSHJ1YXKJ",
        "outputId": "3758f7ed-afe8-4b09-c820-0fb9d0061090"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'text': 'Okay, I can do that. First, I need to identify the first 14 odd '\n",
            "         'prime numbers. Remember that a prime number is a number greater than '\n",
            "         '1 that has only two divisors: 1 and itself. Also, by definition, 2 '\n",
            "         'is the only even prime number, so we need to find the next 14 odd '\n",
            "         'primes after 2.\\n'\n",
            "         '\\n'\n",
            "         'Then, I will calculate the sum of those numbers.\\n'\n",
            "         '\\n'}\n",
            "{'executable_code': {'code': 'primes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, '\n",
            "                             '37, 41, 43, 47]\\n'\n",
            "                             'sum_of_primes = sum(primes)\\n'\n",
            "                             \"print(f'{primes=}')\\n\"\n",
            "                             \"print(f'{sum_of_primes=}')\\n\",\n",
            "                     'language': 'PYTHON'}}\n",
            "{'code_execution_result': {'outcome': 'OUTCOME_OK',\n",
            "                           'output': 'primes=[3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
            "                                     '31, 37, 41, 43, 47]\\n'\n",
            "                                     'sum_of_primes=326\\n'}}\n",
            "{'text': 'The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, '\n",
            "         '31, 37, 41, 43, and 47. Their sum is 326.\\n'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This response contains multiple parts, including an opening and closing text part that represent regular responses, an executable_code part that represents generated code and a code_execution_result part that represents the results from running the generated code.\n",
        "\n",
        "You can explore them individually."
      ],
      "metadata": {
        "id": "vUwMbvCaYoNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "    if part.text:\n",
        "        display(Markdown(part.text))\n",
        "    elif part.executable_code:\n",
        "        display(Markdown(f'```python\\n{part.executable_code.code}\\n```'))\n",
        "    elif part.code_execution_result:\n",
        "        if part.code_execution_result.outcome != 'OUTCOME_OK':\n",
        "            display(Markdown(f'## Status {part.code_execution_result.outcome}'))\n",
        "\n",
        "        display(Markdown(f'```\\n{part.code_execution_result.output}\\n```'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "IcWPEBpgYm9k",
        "outputId": "e4638ac7-6e7e-4e2c-adbd-4dc6901a4b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, I can do that. First, I need to identify the first 14 odd prime numbers. Remember that a prime number is a number greater than 1 that has only two divisors: 1 and itself. Also, by definition, 2 is the only even prime number, so we need to find the next 14 odd primes after 2.\n\nThen, I will calculate the sum of those numbers.\n\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\nprimes = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes = sum(primes)\nprint(f'{primes=}')\nprint(f'{sum_of_primes=}')\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```\nprimes=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\nsum_of_primes=326\n\n```"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The first 14 odd prime numbers are 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, and 47. Their sum is 326.\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explaining code\n",
        "The Gemini family of models can explain code to you too. In this example, you pass a bash script and ask some questions."
      ],
      "metadata": {
        "id": "sinsiRudY0T2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model='gemini-2.0-flash',\n",
        "    contents=explain_prompt)\n",
        "\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 482
        },
        "id": "yFP03FVMY1fR",
        "outputId": "d872d55d-4624-4c63-9b18-c4666d9146ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file is a **Bash script** designed to **enhance your command-line prompt with Git repository information**.  It customizes your prompt to display details about the current Git branch, its status (ahead/behind remote, changes, etc.), and other useful information directly in your terminal.\n\nHere's a breakdown:\n\n*   **Purpose:**  To provide real-time Git status at a glance within your terminal prompt. This helps you stay aware of your repository's state without having to run `git status` constantly.\n*   **How it works:**\n    *   It defines functions that fetch and format Git information.\n    *   It uses color codes to make the information easily readable.\n    *   It supports customization through themes and configuration variables.\n    *   It modifies the `PS1` environment variable (which controls the appearance of your prompt) to include the Git status.\n    *   It works in both bash and zsh (with some compatibility adjustments).\n*   **Why use it?**\n    *   **Improved workflow:**  Quickly see the status of your Git repository without running extra commands.\n    *   **Context awareness:**  Always know which branch you're on and whether you have uncommitted changes.\n    *   **Customization:**  Tailor the prompt to show the information you find most relevant and in a style that suits you.\n    *   **Efficiency:** Save time and effort by having Git status readily available in your prompt.\n*   **How to use it:**\n\n    1.  You would typically source this script in your `.bashrc` or `.zshrc` file. This means adding a line like `source /path/to/this/script.sh` to the end of your shell configuration file.\n    2.  You might need to install/configure a `prompt_callback` function.\n    3.  You could customize the prompt's appearance by setting environment variables like `GIT_PROMPT_THEME`, `GIT_PROMPT_START`, `GIT_PROMPT_END`, etc., *before* sourcing the script.\n\nIn short, this script turns your terminal prompt into a Git status dashboard, providing you with valuable information without needing to run separate commands.\n"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6VENIqXQMafG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
